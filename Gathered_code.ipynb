{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project description\n",
    "Urban mobility significantly contributes to global environmental impact. With the growth in urbanareas worldwide, cities are implementing policies to focus on sustainable urban and mobility devel-opment. Urban areas exhibit high variability in structure, economic output, mobility behaviour,among others. Pinpointing to the exact causes of emissions is impossible. However, understandingthe relationship of mobility with infrastructure resources and other urban indicators may help gov-ernments predict long-term mobility behaviour and intervene with future pathways into sustainablemobility options. </br>\n",
    "\n",
    "Project goals: You are tasked to identify how mobility is related to the infrastructural form of acity and across multiple cities. Use your findings toward helping governments transform mobilitywithin cities for reducing environmental impacts of mobility use.\n",
    "Data: </br>\n",
    "1. Local city-wide indicators of urban mobility: Multi-city Traffic Datasetprovided publicly byThe Institute for Transport Planning and Systems at ETH Zurich.\n",
    "2. Global city-wide indicators of urban mobility and environmental impacts:Urban TypologiesProject[Reference]3.Get indicators for infrastructure resources from OSMNX (street networks) and PYROSM(amenities and points of interest) python packages.High-level project goals1.Explore, Investigate, and Visualize various factors of the mobility data across cities of interest.2.Describe and incorporate additional data sources that you will use to help you understandrelationships between mobility and the built environment.\n",
    "3. Identify correlations between mobility and the built environment.4.Perform clustering or LISA analysis on city streets where congestion may be related stronglywith the presence of certain amenities. Think about how that may vary with distance toamenities.\n",
    "4. Or, train and evaluate models using the mobility data (either as predictors or as the primaryresponse in some fashion) and your data sources.\n",
    "6. Use and interpret your models to discuss the causes and correlations of or effects due todifferences in congestion across cities or within a city of your choice.\n",
    "7. Aggregate city-wide findings to correlate with environmental indicators used in data set (2) -urban typology - to comment on policy initiatives of the governments. (Think how you mayaggregate city-wide findings to a single score for the whole city to compare with indicatorsused in data set \n",
    "\n",
    "(2))Note: Not all data may be available for all cities. Hence, conduct this analysis for at least5citiesoverlapping between sets1and2in the data. You are welcome to do the analysis in many morecities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTD-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSMNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/detectors_public.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dc404455499b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mUTD19\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data/detectors_public.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# read big file with all cities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnewcitylist\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;34m'munich'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rotterdam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'frankfurt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hamburg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'zurich'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# find only our cities for now, store in dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mUTD19Cities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUTD19\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUTD19\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcitycode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewcitylist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# UTD19Cities is dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gds\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gds\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gds\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gds\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gds\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/detectors_public.csv'"
     ]
    }
   ],
   "source": [
    "UTD19 = pd.read_csv('Data/detectors_public.csv') # read big file with all cities \n",
    "newcitylist= [ 'munich', 'rotterdam', 'frankfurt', 'hamburg', 'zurich'] # find only our cities for now, store in dataframe\n",
    "UTD19Cities = UTD19.loc[UTD19.citycode.isin(newcitylist)] # UTD19Cities is dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSMNX Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newcitylist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-362d5a3091ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mGraphList\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mGraphs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetGraphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewcitylist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUTD19Cities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'newcitylist' is not defined"
     ]
    }
   ],
   "source": [
    "def GetGraphs(clist, df):\n",
    "    # function designed to extract bbox and create graphs with bbox\n",
    "    GraphList = []\n",
    "    for i in clist:\n",
    "        maxlonglt = df.loc[df.citycode == i, ['long', 'lat']].max() # maxlonglat[0] = long, 1 lat\n",
    "        minlonglt = df.loc[df.citycode == i, ['long', 'lat']].min() #\n",
    "        north = maxlonglt[1] \n",
    "        south = minlonglt[1]\n",
    "        east = maxlonglt[0]\n",
    "        west = minlonglt[0]\n",
    "        A = ox.graph.graph_from_bbox(north, south, east, west, network_type='drive')\n",
    "        GraphList.append(A)\n",
    "    return GraphList\n",
    "        \n",
    "Graphs = GetGraphs(newcitylist, UTD19Cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of plotting a graph\n",
    "G1 = ox.project_graph(Graphs[1])\n",
    "fig, ax = ox.plot_graph(G1, node_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge attributes and node data\n",
    "Rotterdam = Graphs[1]\n",
    "node_data, edge_attributes = ox.graph_to_gdfs(Rotterdam) # Get node info and edge info\n",
    "\n",
    "NameLink = pd.DataFrame(edge_attributes[['osmid','name']]) # only extract the osmid and road name\n",
    "NameLink.rename({'name': 'road'}, inplace = True, axis = 1) # The Graph network has more raods in it than UTD\n",
    "# some entries are a list of street names \n",
    "NameLink = NameLink.explode('road') # use this to create entry with same values, so road is not a list anymore\n",
    "#NameLink = NameLink.explode('osmid') # some roads have two osmid -> explode\n",
    "\n",
    "URotterdam = UTD19Cities.loc[UTD19Cities.citycode == 'rotterdam', ['detid', 'road']] #only get detid and road name\n",
    "\n",
    "TranslateTable = pd.merge(NameLink, URotterdam, on = 'road', how = 'right')\n",
    "TranslateTable.head() # links osmid and detid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# way to add colors to edges\n",
    "x = [1 if x%2 == 0 else 0 for x in edge_attributes.index] # create some random atribute\n",
    "\n",
    "edge_attributes['X']=x # set att\n",
    "\n",
    "WithX = ox.graph_from_gdfs(node_data, edge_attributes, graph_attrs={'edge_attr':'X'}) # graph_attrs and edge works it is safed! and able to call back\n",
    "WithX.graph['crs'] = Rotterdam.graph['crs'] # need crs to plot\n",
    "ec = ox.plot.get_edge_colors_by_attr(WithX, attr='length', cmap='viridis') # set color scale by atribute (now length could be any other variable)\n",
    "\n",
    "G1 = ox.project_graph(WithX)\n",
    "fig, ax = ox.plot_graph(G1, node_size=1, edge_color=ec) # edgecolor is stored in ec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban typologies import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citylist = ['Munich', 'Rotterdam-Hague', 'Frankfurt', 'Hamburg', 'Zurich']\n",
    "UrbanTypo = pd.read_excel('Data/FINAL-COMBINED-DATASET.xlsx')\n",
    "Cities = UrbanTypo.loc[(UrbanTypo.City.isin(citylist))]\n",
    "Cities.reset_index(inplace = True)\n",
    "Cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fabians Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rotterdam = Graphs[1]\n",
    "node_data, edge_attributes = ox.graph_to_gdfs(Rotterdam) # Get node info and edge info\n",
    "#switch first and second column\n",
    "colnames=[]\n",
    "for i, names in enumerate(edge_attributes.columns):\n",
    "    colnames.append(names)\n",
    "colnames[0]='name'\n",
    "colnames[1]='osmid'\n",
    "#print(colnames)\n",
    "edge_attributes=edge_attributes.reindex(columns=colnames)\n",
    "edge_attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfea=edge_attributes#chiller, want korter\n",
    "dfea.loc[:,['name']]=np.nan_to_num(dfea.loc[:,['name']])\n",
    "\n",
    "#create list to log whether a row contains a list for osmid\n",
    "listornot=[]\n",
    "for i in range(len(dfea.name)):\n",
    "    if type(dfea.iloc[i,0]) == list:\n",
    "        listornot.append(True)\n",
    "    else:\n",
    "        listornot.append(False)\n",
    "#now append to df!\n",
    "dfea['list?']=listornot\n",
    "dfea.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save lists \n",
    "dfea_lists=dfea.loc[(dfea['list?'] == True)]\n",
    "#save nonlists\n",
    "dfea_nonlists=dfea.loc[(dfea['list?'] == False)]\n",
    "dfea_lists=dfea_lists.reset_index()\n",
    "#dfea_lists.drop(['index'])\n",
    "dfea_lists=dfea_lists.iloc[:,1:17]\n",
    "dfea_lists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thrid try, also put appends in loop.\n",
    "cdict = {name: [] for name in (dfea_lists.columns)} #make dict that consists out of lists equal to columns of df to fill below, with right names\n",
    "\n",
    "for i in range (len(dfea_lists.name)):\n",
    "    for j in range(len(dfea_lists.name[i])):\n",
    "        ab=dfea_lists.name[i]\n",
    "        cdict['name'].append(ab[j])\n",
    "        for k, names_to_append in enumerate(dfea_lists.columns[1:len(dfea_lists.columns)]):\n",
    "            l=k+1\n",
    "            cdict[names_to_append].append(dfea_lists.iloc[i,l])\n",
    "        \n",
    "dfnew=pd.DataFrame.from_dict(cdict)\n",
    "dfnew.head()\n",
    "#works! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=[dfea_nonlists,dfnew]\n",
    "final_set=pd.concat(frames)\n",
    "final_set.head() # this is the final mergable set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYROSM Points of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de filepaths zin nu specifiek voor mijn computer, moet je zelf even aanpassen naar waar die van jou staan\n",
    "fp_rotterdam =\"Documents/EPA/Urban Data Science/Final assignment/data/rotterdam.osm.pbf\"\n",
    "fp_munich =\"C:/Users/hanna/Documents/EPA/Urban Data Science/Final assignment/data/munich.osm.pbf\"\n",
    "fp_zurich =\"C:/Users/hanna/Documents/EPA/Urban Data Science/Final assignment/data/zurich.osm.pbf\"\n",
    "fp_hamburg =\"C:/Users/hanna/Documents/EPA/Urban Data Science/Final assignment/data/hamburg.osm.pbf\"\n",
    "fp_frankfurt =\"C:/Users/hanna/Documents/EPA/Urban Data Science/Final assignment/data/frankfurt.osm.pbf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [fp_rotterdam, fp_munich, fp_zurich, fp_hamburg, fp_frankfurt]:\n",
    "    # Initiliaze with bounding box\n",
    "    osm = OSM(filepath=i)\n",
    "\n",
    "    # Read POIs with custom filter \n",
    "    my_filter={'building': True} #\"amenity\": True, 'shop': True, }\n",
    "    pois = osm.get_pois(custom_filter=my_filter)\n",
    "\n",
    "    # Merge poi type information into a single column\n",
    "    #pois[\"shop\"] = pois[\"shop\"].fillna(' ')\n",
    "    #pois[\"amenity\"] = pois[\"amenity\"].fillna(' ')\n",
    "    pois[\"building\"] = pois[\"building\"].fillna(' ')\n",
    "    pois[\"poi_type\"] = pois[\"building\"] #+ pois[\"amenity\"] + pois[\"shop\"] \n",
    "\n",
    "    # Plot\n",
    "    ax = pois.plot(column=\"poi_type\", legend=True, markersize=4, figsize=(14,8), legend_kwds=dict(loc='upper left', ncol=2, bbox_to_anchor=(1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean flow per Detid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv-file per City\n",
    "data_frankfurt = pd.read_csv('data\\\\UTD_f.csv')\n",
    "data_hamburg = pd.read_csv('data\\\\UTD_h.csv')\n",
    "data_munich = pd.read_csv('data\\\\UTD_m.csv')\n",
    "data_rotterdam = pd.read_csv('data\\\\UTD_r.csv')\n",
    "data_zurich = pd.read_csv('data\\\\UTD_z.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tables with average per detid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frankfurt_detid_average = (data_frankfurt.groupby('detid').flow.mean()).to_frame(name = None)\n",
    "hamburg_detid_average = (data_hamburg.groupby('detid').flow.mean()).to_frame(name = None)\n",
    "munich_detid_average = (data_munich.groupby('detid').flow.mean()).to_frame(name = None)\n",
    "rotterdam_detid_average = (data_rotterdam.groupby('detid').flow.mean()).to_frame(name = None)\n",
    "zurich_detid_average = (data_zurich.groupby('detid').flow.mean()).to_frame(name = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the name of the city to each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frankfurt_detid_average['city'] = 'frankfurt'\n",
    "hamburg_detid_average['city'] = 'hamburg'\n",
    "munich_detid_average['city'] = 'munich'\n",
    "rotterdam_detid_average['city'] =  'rotterdam'\n",
    "zurich_detid_average['city'] = 'zurich'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show example of table\n",
    "zurich_detid_average.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the different tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_perdetid = pd.concat([frankfurt_detid_average,\n",
    "                                  hamburg_detid_average,\n",
    "                                  munich_detid_average,\n",
    "                                  rotterdam_detid_average,\n",
    "                                  zurich_detid_average])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show example of table\n",
    "Mean_perdetid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if number of unique detid is equal to total length of the table, this indicates that there are no overlapping detids\n",
    "if  (len(Mean_perdetid) == Mean_perdetid.index.nunique()):\n",
    "    print('all detids are unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Result\n",
    "result_unique  = Mean_perdetid.to_csv('data\\\\UTD_Mean_Flow_per_detid.csv', index = True)#exporting data frame as csv file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
